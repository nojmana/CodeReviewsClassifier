{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.4.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.13.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kinga/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/kinga/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(['stopwords', 'punkt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying results of GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' source https://www.kaggle.com/grfiv4/displaying-the-results-of-a-grid-search '''\n",
    "\n",
    "def GridSearch_table_plot(grid_clf, param_name,\n",
    "                          num_results=15,\n",
    "                          negative=True,\n",
    "                          display_all_params=True):\n",
    "\n",
    "    '''Display grid search results\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "\n",
    "    grid_clf           the estimator resulting from a grid search\n",
    "                       for example: grid_clf = GridSearchCV( ...\n",
    "\n",
    "    param_name         a string with the name of the parameter being tested\n",
    "\n",
    "    num_results        an integer indicating the number of results to display\n",
    "                       Default: 15\n",
    "\n",
    "    negative           boolean: should the sign of the score be reversed?\n",
    "                       scoring = 'neg_log_loss', for instance\n",
    "                       Default: True\n",
    "\n",
    "    display_all_params boolean: should we print out all of the parameters, not just the ones searched for?\n",
    "                       Default: True\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "\n",
    "    GridSearch_table_plot(grid_clf, \"min_samples_leaf\")\n",
    "\n",
    "                          '''\n",
    "    from matplotlib      import pyplot as plt\n",
    "    from IPython.display import display\n",
    "    import pandas as pd\n",
    "\n",
    "    clf = grid_clf.best_estimator_\n",
    "    clf_params = grid_clf.best_params_\n",
    "    if negative:\n",
    "        clf_score = -grid_clf.best_score_\n",
    "    else:\n",
    "        clf_score = grid_clf.best_score_\n",
    "    clf_stdev = grid_clf.cv_results_['std_test_score'][grid_clf.best_index_]\n",
    "    cv_results = grid_clf.cv_results_\n",
    "\n",
    "    print(\"best parameters: {}\".format(clf_params))\n",
    "    print(\"best score:      {:0.5f} (+/-{:0.5f})\".format(clf_score, clf_stdev))\n",
    "    if display_all_params:\n",
    "        import pprint\n",
    "        pprint.pprint(clf.get_params())\n",
    "\n",
    "    # pick out the best results\n",
    "    # =========================\n",
    "    scores_df = pd.DataFrame(cv_results).sort_values(by='rank_test_score')\n",
    "\n",
    "    best_row = scores_df.iloc[0, :]\n",
    "    if negative:\n",
    "        best_mean = -best_row['mean_test_score']\n",
    "    else:\n",
    "        best_mean = best_row['mean_test_score']\n",
    "    best_stdev = best_row['std_test_score']\n",
    "    best_param = best_row['param_' + param_name]\n",
    "\n",
    "    # display the top 'num_results' results\n",
    "    # =====================================\n",
    "    display(pd.DataFrame(cv_results) \\\n",
    "            .sort_values(by='rank_test_score').head(num_results))\n",
    "\n",
    "    # plot the results\n",
    "    # ================\n",
    "    scores_df = scores_df.sort_values(by='param_' + param_name)\n",
    "\n",
    "    if negative:\n",
    "        means = -scores_df['mean_test_score']\n",
    "    else:\n",
    "        means = scores_df['mean_test_score']\n",
    "    stds = scores_df['std_test_score']\n",
    "    params = scores_df['param_' + param_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gerrit_file = 'gerrit-wireshark-train-test-v4.xlsx'\n",
    "indexes_file = \"indexes.csv\"\n",
    "seed = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset\n",
    "Reads data from excel as a list of dicts with two keys: \"message\" and \"purpose\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_excel(file_name, sheet_name):\n",
    "    data = pd.read_excel(file_name, sheet_name=sheet_name, index_col=None, usecols=['message', 'purpose'])\n",
    "    return data.to_dict(orient='record')\n",
    "\n",
    "\n",
    "data_set = read_excel(gerrit_file, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate indexes and save to file\n",
    "The indexes.csv file has following structure: \n",
    "<br>[train_indexes]\n",
    "<br>[test_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_indexes(length, index, seed):\n",
    "    random.seed(seed)\n",
    "    np_array = np.arange(len(data_set))\n",
    "    random.shuffle(np_array)\n",
    "    split_index = int(index*len(data_set))\n",
    "    return np_array[:split_index], np_array[split_index:]\n",
    "\n",
    "\n",
    "train_indexes, test_indexes = generate_indexes(len(data_set), 0.9, seed)\n",
    "indexes = [train_indexes, test_indexes]\n",
    "\n",
    "with open(indexes_file, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and testing set by\n",
    "- splitting data according to previously generated to file indexes \n",
    "- shuffling data and splitting its randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_indexes(indexes_file):\n",
    "    with open(indexes_file, \"r\", newline=\"\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        train_indexes = next(reader)\n",
    "        test_indexes = next(reader)\n",
    "        return train_indexes, test_indexes\n",
    "\n",
    "\n",
    "def split_set_with_indexes(data_set, train_indexes, test_indexes):\n",
    "    train_set = [data_set[int(i)] for i in train_indexes]\n",
    "    test_set = [data_set[int(i)] for i in test_indexes]\n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "train_indexes, test_indexes = read_indexes(indexes_file)    \n",
    "train_set, test_set = split_set_with_indexes(data_set, train_indexes, test_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_set_randomly(data_set, index, seed):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(data_set)\n",
    "\n",
    "    split_index = int(index*len(data_set))\n",
    "    return data_set[:split_index], data_set[split_index:]\n",
    "\n",
    "train_set, test_set = split_set_randomly(data_set, 0.9, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset to x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_to_x_y(data_set):\n",
    "    data_set_x = list([row['message'] for row in data_set])\n",
    "    data_set_y = list([row['purpose'] for row in data_set])\n",
    "\n",
    "    return data_set_x, data_set_y\n",
    "\n",
    "\n",
    "train_set_x, train_set_y = split_dataset_to_x_y(train_set)\n",
    "test_set_x, test_set_y = split_dataset_to_x_y(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BOW representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "def preprocess_data(dataset):\n",
    "    tokenizer = RegexpTokenizer(\"[a-zA-Z@]+\")\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stop = stopwords.words('english')\n",
    "    excluding = ['against', 'not', 'don', \"don't\", 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n",
    "                 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",\n",
    "                 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "                 \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    stop = [words for words in stop if words not in excluding]\n",
    "\n",
    "    for index, message in enumerate(dataset):\n",
    "        message = tokenizer.tokenize(message)\n",
    "        dataset[index] = \" \".join([stemmer.stem(w) for w in message if w not in stop])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def count_words_frequencies(dataset):\n",
    "    frequencies = {}\n",
    "    for message in dataset:\n",
    "        words = nltk.word_tokenize(message)\n",
    "        for word in words:\n",
    "            if word not in frequencies.keys():\n",
    "                frequencies[word] = 1\n",
    "            else:\n",
    "                frequencies[word] += 1\n",
    "    return frequencies\n",
    "\n",
    "\n",
    "def create_vector(messages, frequent_words):\n",
    "    bow_model = []\n",
    "    for message in messages:\n",
    "        vector = []\n",
    "        for word in frequent_words:\n",
    "            if word in nltk.word_tokenize(message):\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        bow_model.append(vector)\n",
    "    return np.asarray(bow_model)\n",
    "\n",
    "\n",
    "def create_bow_model(data_set_x, frequent_words_count):\n",
    "    data_set_x = preprocess_data(data_set_x)\n",
    "    word_frequencies = count_words_frequencies(data_set_x)\n",
    "    frequent_words = heapq.nlargest(frequent_words_count, word_frequencies, key=word_frequencies.get)\n",
    "    bow_model = create_vector(data_set_x, frequent_words)\n",
    "    return bow_model\n",
    "\n",
    "\n",
    "frequent_words_count = 100\n",
    "train_bow_model = create_bow_model(train_set_x, frequent_words_count)\n",
    "test_bow_model = create_bow_model(test_set_x, frequent_words_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def measure_efficiency(y, predictions):\n",
    "    print('Accuracy Score : ', str(accuracy_score(y, predictions)))\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, seed, train_bow_model, train_set_y, test_bow_model, test_set_y):\n",
    "        self.seed = seed\n",
    "        self.train_bow_model = train_bow_model\n",
    "        self.train_set_y = train_set_y\n",
    "        self.test_bow_model = test_bow_model\n",
    "        self.test_set_y = test_set_y\n",
    "\n",
    "    def logistic_regression(self):\n",
    "        print('Logistic regression')\n",
    "        log_reg = LogisticRegression(random_state=self.seed, max_iter=1000000, solver='liblinear', C=0.1)\n",
    "        log_reg.fit(self.train_bow_model, self.train_set_y)\n",
    "        predictions = log_reg.predict(self.test_bow_model)\n",
    "        measure_efficiency(self.test_set_y, predictions)\n",
    "\n",
    "    def grid_search_logistic_regression(self):\n",
    "        param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                      'solver': ['liblinear', 'newton-cg', 'sag', 'saga', 'lbfgs']}\n",
    "        grid_search = GridSearchCV(LogisticRegression(random_state=self.seed, max_iter=1000000), param_grid, verbose=1)\n",
    "        grid_search.fit(self.train_bow_model, self.train_set_y)\n",
    "        predictions = grid_search.predict(self.test_bow_model)\n",
    "        measure_efficiency(self.test_set_y, predictions)\n",
    "        GridSearch_table_plot(grid_search, \"C\", graph=False, negative=False)\n",
    "\n",
    "    def random_forest(self, estimators_number):\n",
    "        print('Random forest')\n",
    "        rfc = RandomForestClassifier(estimators_number, random_state=self.seed)\n",
    "        rfc.fit(self.train_bow_model, self.train_set_y)\n",
    "        predictions = rfc.predict(self.test_bow_model)\n",
    "        measure_efficiency(self.test_set_y, predictions)\n",
    "\n",
    "    def grid_search_random_forest(self):\n",
    "        param_grid = {'n_estimators': [1, 10, 100, 1000]}\n",
    "        grid_search = GridSearchCV(RandomForestClassifier(random_state=self.seed), param_grid, verbose=1)\n",
    "        grid_search.fit(self.train_bow_model, self.train_set_y)\n",
    "        predictions = grid_search.predict(self.test_bow_model)\n",
    "        measure_efficiency(self.test_set_y, predictions)\n",
    "        GridSearch_table_plot(grid_search, \"n_estimators\", graph=False, negative=False)\n",
    "\n",
    "    def decision_tree(self):\n",
    "        print('Decision tree')\n",
    "        decision_tree = DecisionTreeClassifier(random_state=self.seed)\n",
    "        decision_tree.fit(self.train_bow_model, self.train_set_y)\n",
    "        predictions = decision_tree.predict(self.test_bow_model)\n",
    "        measure_efficiency(self.test_set_y, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression\n",
      "Accuracy Score :  0.64\n",
      "Random forest\n",
      "Accuracy Score :  0.62\n",
      "Decision tree\n",
      "Accuracy Score :  0.52\n"
     ]
    }
   ],
   "source": [
    "classifier = Classifier(seed, train_bow_model, train_set_y, test_bow_model, test_set_y)\n",
    "\n",
    "classifier.logistic_regression()\n",
    "classifier.random_forest(100)\n",
    "classifier.decision_tree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
